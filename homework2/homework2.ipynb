{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "#### Gavin Martin \n",
    "#### March 12, 2020\n",
    "#### CS 344 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'am': 0.99, 'spam': 0.99, 'i': 0.5, 'samiam': 0.4, 'not': 0.4, 'that': 0.4, 'do': 0.3333333333333333, 'like': 0.3333333333333333, 'and': 0.01}\n",
      "Test 1 Spam probability is 0.88\n",
      "\n",
      "{'i': 0.5, 'do': 0.3333333333333333, 'like': 0.3333333333333333, 'green': 0.01, 'eggs': 0.01, 'and': 0.01, 'ham': 0.01}\n",
      "Test 2 Spam probability is 0.0\n",
      "\n",
      "{'spam': 0.99, 'i': 0.5, 'but': 0.4, 'only': 0.4, 'if': 0.4, 'it': 0.4, 'is': 0.4, 'not': 0.4, 'made': 0.4, 'by': 0.4, 'that': 0.4, 'spamiam': 0.4, 'because': 0.4, 'no': 0.4, 'like': 0.3333333333333333}\n",
      "Test 3 Spam probability is 0.2762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define global corpuses for spam filtering \n",
    "good_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "spam_corpus = [[\"i\", \"am\", \"spam\", \"spam\", \"i\", \"am\"], [\"i\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "\n",
    "# this creates dictionary of word and its frequency in a corpus\n",
    "def create_word_occurrence_dict(corpus):\n",
    "    new_dict = {}\n",
    "    for message in corpus:\n",
    "        for word in message:\n",
    "            if word in new_dict:\n",
    "                #increment frequency\n",
    "                new_dict[word] += 1\n",
    "            else:\n",
    "                new_dict.update({word: 1})\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# get word spam probability of individual word given good and bad corpus\n",
    "def get_word_spam_probability(word, good_occurrence_dict, spam_occurrence_dict):\n",
    "    good_freq = 0\n",
    "    bad_freq = 0\n",
    "    # get num occurences\n",
    "    if word in good_occurrence_dict:\n",
    "        good_freq = good_occurrence_dict[word] * 2\n",
    "    if word in spam_occurrence_dict:\n",
    "        bad_freq = spam_occurrence_dict[word]\n",
    "    # get num messages in corpus\n",
    "    num_good_messages = len(good_corpus)\n",
    "    num_bad_messages = len(spam_corpus)\n",
    "    # if it occurs more than 1x\n",
    "    if good_freq + bad_freq > 1:\n",
    "        # get probability\n",
    "        word_value = max(.01, min(.99, min(1.0, bad_freq / num_bad_messages) / (\n",
    "                min(1.0, good_freq/num_good_messages) + min(1.0, bad_freq/num_bad_messages))))\n",
    "    else:\n",
    "        # if not found, it has probability of .4\n",
    "        word_value = .4\n",
    "    return word_value\n",
    "\n",
    "\n",
    "# get probability dictionary of all words in the message\n",
    "def get_word_probability_dict(message):\n",
    "    # Analyze each word from message\n",
    "    global good_corpus\n",
    "    global spam_corpus\n",
    "    # get word frequency dictionaries from both corpus\n",
    "    good_occurrence_dict = create_word_occurrence_dict(good_corpus)\n",
    "    spam_occurrence_dict = create_word_occurrence_dict(spam_corpus)\n",
    "    # split up message\n",
    "    word_list = message.split()\n",
    "    new_dict = {}\n",
    "    for word in word_list:\n",
    "        # get the probabilities and add them to dictionary\n",
    "        probability = get_word_spam_probability(word, good_occurrence_dict, spam_occurrence_dict)\n",
    "        new_dict.update({word: probability})\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# This gets the most interesting words (ones with values furthest from .5 and creates a dictionary with them\n",
    "def get_most_interesting_words(word_dict):\n",
    "    new_dict = {}\n",
    "    max_length = 15\n",
    "    # if the message is shorter than 15 words, that becomes max length\n",
    "    if len(word_dict) < 15:\n",
    "        max_length = len(word_dict)\n",
    "    for x in range(max_length):\n",
    "        # get the word probabilities\n",
    "        probabilities = list(word_dict.values())\n",
    "        # get the words themselves\n",
    "        words = list(word_dict.keys())\n",
    "        # get the max word from the current dictionary\n",
    "        max_word = words[probabilities.index(max(probabilities))]\n",
    "        # add it to new dictionary of important words\n",
    "        new_dict.update({max_word: word_dict[max_word]})\n",
    "        # delete previous max word so you can get next max\n",
    "        word_dict.pop(max_word)\n",
    "    return new_dict\n",
    "\n",
    "# This gets the combined probability of a dictionary of word / probability pairs\n",
    "def get_combined_probability(word_dict):\n",
    "    product = 1\n",
    "    inverse_product = 1\n",
    "    for word in word_dict:\n",
    "        product *= word_dict[word]\n",
    "        inverse_product *= (1 - word_dict[word])\n",
    "    return product / (product + inverse_product)\n",
    "\n",
    "\n",
    "# Determines probability of spam message\n",
    "def determine_spam_probability(message):\n",
    "    # Gets the probability that each token is spam\n",
    "    token_spam_probability_dict = get_word_probability_dict(message)\n",
    "    # Gets the most interesting tokens from the message\n",
    "    interesting_tokens_dict = get_most_interesting_words(token_spam_probability_dict)\n",
    "    print(interesting_tokens_dict) # To show this works as intended\n",
    "    # this gets combined probability of every word in message\n",
    "    message_spam_probability = get_combined_probability(interesting_tokens_dict)\n",
    "    # this rounds the decimal to the 4th place\n",
    "    return round(message_spam_probability, 4)\n",
    "\n",
    "\n",
    "test1_spam = \"i am samiam and i do not like that spam\"\n",
    "test2_good = \"i do i do like green eggs and ham \"\n",
    "test3_mixed = \"i like green eggs and ham but only if it is not made by that spamiam because i do not like spam no spam no spam\"\n",
    "print(\"\\n\")\n",
    "test1_probability = determine_spam_probability(test1_spam)\n",
    "print(\"Test 1 Spam probability is \" + str(test1_probability) + \"\\n\")\n",
    "test2_probability = determine_spam_probability(test2_good)\n",
    "print(\"Test 2 Spam probability is \" + str(test2_probability) + \"\\n\")\n",
    "test3_probability = determine_spam_probability(test3_mixed)\n",
    "print(\"Test 3 Spam probability is \" + str(test3_probability) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graham argues that this is a Baysian approach to SPAM. What makes it Bayesian?\n",
    "\n",
    "- It is a Bayesian approach because we are taking into account the Bayesian network that is created through the corpuses which are hashed into probability distrubtions. We are taking the Spam and Not Spam corpus and making a large distrubtion table through the hashes which we then use to calculate the joint probability of each word in the phrases. We are effectivly calculating a cause and effect where the probability of this spam email is calculated using the effect of previous spam and not spam emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probability import BayesNet, enumeration_ask, elimination_ask, gibbs_ask\n",
    "\n",
    "# Utility variables\n",
    "T, F = True, False\n",
    "\n",
    "# From AIMA code (probability.py) - Fig. 14.2 - burglary example\n",
    "cloudy = BayesNet([\n",
    "    ('Cloudy', '', 0.5),\n",
    "    ('Sprinkler', 'Cloudy', {T: .1, F: .5}),\n",
    "    ('Rain', 'Cloudy', {T: .8, F: .2}),\n",
    "    ('WetGrass', 'Sprinkler Rain', {(T, T): 0.99, (T, F): 0.9, (F, T): 0.9, (F, F): 0.0}),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of independent values in the full joint probability distribution for this domain. Assume that no conditional independence relations are known to hold between these values.\n",
    "\n",
    "There are 4 nodes / variables making it 2 * 2 * 2 * 2 or 16 outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of independent values in the Bayesian network for this domain. Assume the conditional independence relations implied by the Bayes network.\n",
    "\n",
    "Cloudy affects all of them and that has 2 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P(Cloudy)\n",
      "False: 0.5, True: 0.5\n",
      "\n",
      "P(Sprinkler | Cloudy)\n",
      "False: 0.9, True: 0.1\n",
      "\n",
      "P(Cloudy | Sprinkler and no Rain)\n",
      "False: 0.952, True: 0.0476\n",
      "\n",
      "P(Wet Grass | sprinkler and cloudy and raining)\n",
      "False: 0.01, True: 0.99\n",
      "\n",
      "P(Cloudy | no wet grass)\n",
      "False: 0.639, True: 0.361\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nP(Cloudy)\")\n",
    "print(enumeration_ask('Cloudy', dict(), cloudy).show_approx())\n",
    "\n",
    "print(\"\\nP(Sprinkler | Cloudy)\")\n",
    "print(enumeration_ask('Sprinkler', dict(Cloudy=True), cloudy).show_approx())\n",
    "\n",
    "print(\"\\nP(Cloudy | Sprinkler and no Rain)\")\n",
    "print(enumeration_ask('Cloudy', dict(Sprinkler=True, Rain=False), cloudy).show_approx())\n",
    "\n",
    "print(\"\\nP(Wet Grass | sprinkler and cloudy and raining)\")\n",
    "print(enumeration_ask(\"WetGrass\", dict(Sprinkler=True, Rain=True, Cloudy=True), cloudy).show_approx())\n",
    "\n",
    "print(\"\\nP(Cloudy | no wet grass)\")\n",
    "print(enumeration_ask(\"Cloudy\", dict(WetGrass=False), cloudy).show_approx())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Cloudy)\n",
    "i. This has .5 and it is independent, so it is .5 for both true and false\n",
    "\n",
    "<.5, .5>\n",
    "\n",
    "P(Sprinker | cloudy)\n",
    "\n",
    "ii. \n",
    "\n",
    "<P(S|C), P(not S|C)>\n",
    "\n",
    "<.1, 1-.1> or \n",
    "\n",
    "<.1 (true), .9 (false)>\n",
    "\n",
    "\n",
    "P(Cloudy| the sprinkler is running and it’s not raining)\n",
    "iii.  \n",
    "<P(C)*P(S|C)*P(not R|C), P(not C)*P(S|not C)*P(not R|not C)>\n",
    "\n",
    "<(.5)*(.1)*(1-.8), (1-.5)*(.5)*(1-.2)>\n",
    "\n",
    "<.010, .2> \n",
    "\n",
    "<.0476 (true), .952 (false)>\n",
    "\n",
    "\n",
    "P(WetGrass | it’s cloudy, the sprinkler is running and it’s raining)\n",
    "iv.   \n",
    "\n",
    "<P(W|s,r) * P(s|c) * P(r|c) * P(c), P(not W|s,r)*P(s|c)*P(c)>\n",
    "\n",
    "<(.99)*(.1)*(.2)*(.5), (1-.99)*(.1)*(.8)*(.5)>\n",
    "\n",
    "<.0396, .0004>\n",
    "\n",
    "<.99 (true), .01 (false)>\n",
    "                \n",
    "\n",
    "\n",
    "P(Cloudy | not Wet grass)\n",
    "\n",
    "<P(C) * P(not W|s,r)*P(s|c)*P(r|c), P(not C) * P(s|not c) * P(r|not c)>\n",
    "<.639 (false), .361 (true)>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
