


What does fairness mean in the context of ML applications? Do you believe that this is an issue in current ML practice?
    -	We don’t report things we think are typical. There is the unconscious bias of the world that we might reflect
        in ML when using some of the world’s data.
    -	This is an issue because it affects the way ML programs learn and work and it reflects our imperfect way of
        interpreting data and processing it.


Describe the basic types of bias identified in the materials.
-	Reporting bias: occurs when the frequency of events, properties, and/or outcomes captured in a data set does not accurately reflect their real-world frequency. This bias can arise because people tend to focus on documenting circumstances that are unusual or especially memorable, assuming that the ordinary can "go without saying.
-	Automation Bias: is a tendency to favor results generated by automated systems over those generated by non-automated systems, irrespective of the error rates of each.
-	Selection Bias - occurs if a data set's examples are chosen in a way that is not reflective of their real-world distribution. Selection bias can take many different forms:
-	Coverage bias: Data is not selected in a representative fashion
-	Non-response bias (or participation bias): Data ends up being unrepresentative due to participation gaps in the data-collection process
-	Sampling bias: Proper randomization is not used during data collection
-	Group attribution bias is a tendency to generalize what is true of individuals to an entire group to which they belong. Two key manifestations of this bias are
o	In-group bias: A preference for members of a group to which you also belong, or for characteristics that you also share
-	Out-group homogeneity bias: A tendency to stereotype individual members of a group to which you do not belong, or to see their characteristics as more uniform
-	Implicit bias occurs when assumptions are made based on one's own mental models and personal experiences that do not necessarily apply more generally
-	confirmation bias, where model builders unconsciously process data in ways that affirm preexisting beliefs and hypotheses
-	Experimenter’s bias -  In some cases, a model builder may actually keep training a model until it produces a result that aligns with their original hypothesis; this is called experimenter's bias





Describe the indicators of dataset bias.

-	There are holes missing in some of the data
-	Bad data that doesn’t make any sense.
-	Data is more precise for certain people, groups, or features when sepearted like male vs female. \
-	Some features are over-represented like the califronia non-randomized validation exercise, you want to make sure everyone/thing is represented equally.



