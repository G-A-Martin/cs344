{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Vision: \n",
    "\n",
    "It has been my dream since starting out in the field of Computer Science to develop a realistic chatbot modeled after myself. I am naturally an introvert, and the idea of developing something that can handle the tedious amounts messages  and emails I get throughout the day sounded like something from science fiction. \n",
    "With this project, I wanted to develop an AI that would learn to talk like me by reading over 5 gigabytes and 5 years of chat history on Facebook. I wanted it to learn to respond to things the same way that I did. My goal was to have something that could pass a Turing test over Facebook in convincing people that my bot was really me. I wanted to create a tool that my great grandkids could use to chat with me long after I’ve passed. My vision was also to create a site after the project ended where people can send messages to my bot that responds over a server, so anyone could talk to me, even when I’m not really there. \n",
    " For the sake of creating a mature and adult-like bot, I decided to use the past 5 years of Facebook data rather than the past 10. For the project, I used Seq2Seq and LSTM, so that it would piece my words together coherently, and remember past messages after sending them.\n",
    "\n",
    "\n",
    "\n",
    "# Background\n",
    "\n",
    " ### Technology Used: \n",
    "\n",
    "In this project, I used LSTM and Seq2Seq recurrent neural networks. LSTM stands for Long Short Term Memory and just like Seq2Seq, it runs on a recurrent neural network. Recurrent neural networks or RNNs for short, are used to help maintain memory in text-processing AI models. A recurrent neural network is like having a neural network run recursively through itself or through a series of copies of itself. These are great for analyzing things like context in a sentence and determining what word should come next, but by itself, a RNN doesn’t hold much long-term memory. \n",
    "\tThis problem can be fixed using LSTM which allows the program to hold long-term dependencies. As the LSTM RNN looks at the incoming data, the neurons are able to choose to pick up new information to remember and forget previously stored pieces of memory. This is really good for having the AI model remember key parts previous of the conversation. \n",
    "\tSeq2Seq words slightly differently. The Seq2Seq model consists of two RNNs. The first one encodes the data into a series of hidden states, the second one decodes it back into something legible. Once encoded, Seq2Seq breaks it up into batches which can be used for training and processing. I created all of these in TensorFlow and NumPy for the sake of getting some experience in something different than Keras. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Inspirational Sources:\n",
    "\n",
    "Throughout this project, I modeled my project after this project: https://bit.ly/2WYTywQ. Despite the fact that this project was completed in 2017, it served as a great guide and tutorial for a large portion of the LSTM and Seq2Seq model training. Many parts of this project do not work anymore because of its age. Facebook has changed its method of data storage, so the parser in this tutorial had to be redone from scratch. The author of this project did not include any testing or validation sets, so they had to be added on in this project for better estimations of loss and progression. \n",
    "I also used the google tutorials given in the guides or at https://bit.ly/2Zbvolz , which served as good reminders for the small things that you should not forget, like the importance of generalizing your data to a validation set to avoid overfitting. Lastly I used a few sources to help clarify how RNNs and Seq2Seq work, and the purpose that LSTMs which can be found here: https://bit.ly/2LsyPfh and here https://bit.ly/3by79Ak. \n",
    "\n",
    "\n",
    "# Implementation\n",
    "\n",
    "\n",
    "### Getting The Data: \n",
    "\n",
    "The first step in implementing an AI model is to get the data for it. For this project, I downloaded all of my Facebook messages in a low-quality JSON format. The low quality only reflects changes in the image quality of the photos sent over messenger, not the actual data itself. After downloading the messages, and unzipping them, I loaded them into my project file and added the path to my .gitIgnore for privacy purposes.  \n",
    "\n",
    "\n",
    "### Parsing\n",
    "\n",
    "I couldn’t just load in all of the data, because grabbing all of my message history and learning from it, would create an AI model that would talk like a combination of everyone I know. Instead, I only grabbed the message and response when I specifically responded directly to a message. I was only interested in grabbing the message that someone sent me, if I had responded to them directly after. Next, I created a dictionary where the key was the message sent to me, and the value for that key was my response to that message. Once I had parsed through all the data that I could, I saved this in a messages.txt file and a conversationDictionary.npy file for later use. Then I created a unique word list from the messages.txt so that every word in my message history could be properly recognized. \n",
    "\n",
    "\n",
    "### Creating the Sets\n",
    "\n",
    "\n",
    "At this point, I separated the dictionary into three different sets. I created a training set with 70% of the conversation dictionary data, a test set with 15% of the data, and a validation set at 15%. With these separate sets appropriately allocated, I made it train on the training set and test for elsewhere.  The only thing that I couldn’t get was matplotlib to work properly. Without this, I couldn’t display the loss with a graph like Colab’s AI tutorials. \n",
    "\n",
    "\n",
    "### Embedding and Tokenization\n",
    "\n",
    "\tOnce everything was in place, I worked on tokenizing the words so that they would be vectors of number ids that linked to words on the encoding hash table. To speed up training, I limited the size of these vectors to a max of 15. Any word vector that didn’t meet the 15 word criteria, had the remainder of the vector filled with <pad> so that it could be the same size and therefore comparable when it came to training. Then I added it to the encoding layer where it trained on the RNN. The decode layer’s output was put through a function that filled the vector of hash ids back to words and then printed them out onto the terminal throughout the training process. I also had an LSTM layer involved so that my AI model could remember basic things that were said in previous messages. \n",
    "\n",
    "\n",
    "### Checkpoints\n",
    "\n",
    "\tFollowing the guide of the tutorial, I created a piece of functionality that created saved checkpoints so that you can see saved versions of the model at different points of the learning process. I configured the settings to save every 10,000 iterations. I also added a portion onto the project that allows you to speak with your model at any point in the process in the app.py file. In the “models/checkpoint” you can change the version of the bot that you want to talk to, which lets you compare previous version to newer smarter versions, which is great for testing for problems like overfitting. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Results\n",
    "\n",
    "As the data trained, I configured the code to print out decoded responses to some hardcoded inputs that I provided so that I could track its learning visually as time progressed. As it trained, its loss went up and down, but overall it shrunk considerably over the course of 7 hours. \n",
    "In stage 1 at less than an hour of training, it was in the infancy stage, at this point it responded to questions with spaces and empty strings. All it had learned was that spaces and padding were the most commonly used characters/words.\n",
    "<img src=\"images/level1.png\">\n",
    "\n",
    "In stage 2, between 1 – 3 hours of training, it began forming words. Here it started spewing out words in random orders without any coherent thought. However, some words were similar to each other, which makes sense, since it was all mashed together into a 300-dimensional embedded matrix.  Here it said things like “how how are you”, “am table on am I”, and “and skiing fly I I I”. \n",
    "<img src=\"images/level2.png\">\n",
    "\n",
    "In stage 3, after 3-5 hours of training, it began using full-phrases. It started using words that went together grammatically and its overall grammar began to improve. It said things like “I think”, and “the test on the table”. At this point my friends brought up questions like “Is it even ethical to kill / shut off a  thinking thing?”. This opened up some discussion but ultimately we had to turn it off at some point, killing the thinking program. \n",
    "<img src=\"images/level3.png\">\n",
    "\n",
    "In stage 4, after 5-7 hours of training, it could respond to questions sometimes, and it knew how to differentiate between English and Spanish. It never became perfect, but most of the sentences were grammatically correct at this point. It knew how to connect ideas to topics of sentences, and it even referenced and remembered previous messages from the conversation chain. \n",
    "<img src=\"images/level4.png\">\n",
    "\n",
    "Ultimately, this AI would never pass the Turing test, but it did show quite a bit of improvement. It learned English at a basic level equivalent to an email from an Indian phisher or to the college application of a Hope student. It wouldn’t fool anyone into thinking it was a human, but it might get someone to click on a dangerous link, and it would definitely be smart enough to get itself admitted to Hope. For instructions on how to run your own version of this bot with your own data, checkout the readme.md at the front of this project. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Implications\n",
    "\n",
    "\n",
    "AI projects like these have major social and ethical implications. AI is undoubtably improving as time goes on, and it is projected to grow exponentially in our future. It may not be long before we don’t know if we’re speaking with a live person or not. We may question every phone call and email we receive, or we may even just assume and accept that they’re likely bots if that’s the way the world goes. We’ve accomplished creating realistic and natural sound digital voice, now all it needs is the right words to say.  Projects like mine can help with that. \n",
    "Before these new advancements enter into our world, we should take a step back and analyze the ramifications and dangers of creating these things. Everything we create immerses itself into the world like dye in water. Soon we can’t even picture life without it. Accepting that technology is never neutral is the first step to engaging in these discussions. You need to look at every possible way that it could go wrong while weighing that out to the potential benefits it could bring. \n",
    "If our chatbot AIs got good enough to fool anyone there would be good and bad consequences. It would be useful if you could connect your own digital personality to your email, Facebook, and to your phone. Then you could theoretically be building relationships with people you otherwise wouldn’t have time to talk to. You could automate helpdesks to give customers and clients immediate support with near-perfect answers. You could automate the complaint department at every retail store, which would mean no one has to get berated and yelled at every day for their living, customers would be yelling at bots thinking they’re human instead. \n",
    "It could also go very wrong. Automating ourselves digitally may take out a part of human interaction in our everyday lives, which may affect us more than we know. It may be frustrating to not have a real live person listening to our problems when we need help, even if the AI does do a good job. If anyone can hook their digital personality up to their emails, phones, and social media accounts, you may never know if you’re talking to real people, which may push an isolating culture to even further extremes. \n",
    "It could even affect the education industry. Professors with AI responding chatbots modeled after their own brains could interact and help students, which would free up much of the professors time. Asking a professors AI brain could be faster than a google search with results that help you more. If this replaces the professor-student relationship too much, it may force students to lose a personal aspect of their education\n",
    "\n",
    "In conclusion, there are both good and bad uses for AI and sometimes there are good and bad consequences for something at the same time. We need to implement our advancements slowly, and we must do so, while observing the consequences from every possible angle. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bibliography\n",
    "\n",
    "\n",
    "On the Dimensionality of Word Embedding by Zi Yin and Yuanyuan Shen\n",
    "-\thttps://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf\n",
    "\n",
    "\n",
    "NLP | Sequence to Sequence Networks| Part 2|Seq2seq Model (EncoderDecoder Model) by Mohammed Ma’amari\n",
    "-\thttps://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1\n",
    "\n",
    "Tuned Version of Seq2Seq Tutorial by Risto Hinno\n",
    "-\thttps://towardsdatascience.com/tuned-version-of-seq2seq-tutorial-ddb64db46e2a\n",
    "\n",
    "\n",
    "\n",
    "Practical PyTorch: Translation with Sequence to Sequence Network and Attention by spro\n",
    "-\thttps://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "\n",
    "How I used Deep Learning To Train A Chatbot To Talk Like Me (Sorta)\n",
    "-\thttps://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me\n",
    "\n",
    "\n",
    "Machine Learning Crash Course by Google\n",
    "-\thttps://developers.google.com/machine-learning/crash-course\n",
    "\n",
    "\n",
    "The Magic of LSTM Neural Networks by Assaad Mowad\n",
    "-\thttps://medium.com/datathings/the-magic-of-lstm-neural-networks-6775e8b540cd\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}